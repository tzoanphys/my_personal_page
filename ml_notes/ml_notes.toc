\babel@toc {english}{}\relax 
\contentsline {chapter}{Introduction}{3}{}%
\contentsline {chapter}{\numberline {1}Statistics You Need for Machine Learning}{4}{}%
\contentsline {section}{\numberline {1.1}Types of Variables}{4}{}%
\contentsline {section}{\numberline {1.2}Descriptive Statistics}{4}{}%
\contentsline {subsection}{\numberline {1.2.1}Mean}{4}{}%
\contentsline {subsection}{\numberline {1.2.2}Median}{5}{}%
\contentsline {subsection}{\numberline {1.2.3}Variance}{5}{}%
\contentsline {subsection}{\numberline {1.2.4}Standard Deviation}{5}{}%
\contentsline {subsection}{\numberline {1.2.5}Covariance}{5}{}%
\contentsline {section}{\numberline {1.3}Correlation}{5}{}%
\contentsline {section}{\numberline {1.4}Probability Theory Basics}{6}{}%
\contentsline {subsection}{\numberline {1.4.1}Joint Probability}{6}{}%
\contentsline {subsection}{\numberline {1.4.2}Conditional Probability}{6}{}%
\contentsline {subsection}{\numberline {1.4.3}Marginal Probability}{6}{}%
\contentsline {section}{\numberline {1.5}Bayes' Theorem: Updating Beliefs from Data}{7}{}%
\contentsline {subsection}{\numberline {1.5.1}Formula}{7}{}%
\contentsline {section}{\numberline {1.6}Likelihood}{8}{}%
\contentsline {section}{\numberline {1.7}Probability Distributions: Shape of Data}{10}{}%
\contentsline {subsection}{\numberline {1.7.1}Discrete Distributions}{10}{}%
\contentsline {paragraph}{Bernoulli Distribution}{10}{}%
\contentsline {paragraph}{Binomial Distribution}{10}{}%
\contentsline {paragraph}{Poisson Distribution}{11}{}%
\contentsline {subsection}{\numberline {1.7.2}Continuous Distributions}{11}{}%
\contentsline {paragraph}{Gaussian (Normal Distribution)}{11}{}%
\contentsline {paragraph}{Exponential Distribution}{11}{}%
\contentsline {paragraph}{Multivariate Gaussian}{12}{}%
\contentsline {section}{\numberline {1.8}Statistical Inference: Learning Parameters From Data}{12}{}%
\contentsline {subsection}{\numberline {1.8.1}Maximum Likelihood Estimation (MLE)}{13}{}%
\contentsline {subsection}{\numberline {1.8.2}Maximum A Posteriori Estimation (MAP)}{13}{}%
\contentsline {section}{\numberline {1.9}Linear Regression: Statistical Interpretation}{15}{}%
\contentsline {section}{\numberline {1.10}Logistic Regression: Probabilistic Classification}{17}{}%
\contentsline {section}{\numberline {1.11}Bias–Variance Tradeoff: Why Models Fail}{19}{}%
\contentsline {chapter}{\numberline {2}Python for Machine Learning}{25}{}%
\contentsline {section}{\numberline {2.1}Supervised Data}{25}{}%
\contentsline {subsection}{\numberline {2.1.1}The Form of Supervised Data}{25}{}%
\contentsline {subsection}{\numberline {2.1.2}The Objective of Supervised Learning}{26}{}%
\contentsline {subsection}{\numberline {2.1.3}Regression vs.~Classification}{27}{}%
\contentsline {paragraph}{Regression}{27}{}%
\contentsline {paragraph}{Classification}{27}{}%
\contentsline {subsection}{\numberline {2.1.4}i.i.d.~Assumption}{27}{}%
\contentsline {subsection}{\numberline {2.1.5}Features and Target Variables}{28}{}%
\contentsline {subsection}{\numberline {2.1.6}The Learning Goal}{28}{}%
\contentsline {section}{\numberline {2.2}Loss Function and Risk Minimization}{29}{}%
\contentsline {subsection}{\numberline {2.2.1}Loss for a Single Example}{30}{}%
\contentsline {subsection}{\numberline {2.2.2}Empirical Risk: Total Loss on the Dataset}{30}{}%
\contentsline {subsection}{\numberline {2.2.3}Risk Minimization}{31}{}%
\contentsline {subsection}{\numberline {2.2.4}Example: Linear Regression Loss}{31}{}%
\contentsline {subsection}{\numberline {2.2.5}Example: Logistic Regression Loss}{32}{}%
\contentsline {subsection}{\numberline {2.2.6}Regularization: Controlling Model Complexity}{33}{}%
\contentsline {subsection}{\numberline {2.2.7}Summarize}{34}{}%
\contentsline {section}{\numberline {2.3}Train / Validation / Test}{34}{}%
\contentsline {subsection}{\numberline {2.3.1}The Three Splits}{34}{}%
\contentsline {paragraph}{1. Training Set}{34}{}%
\contentsline {paragraph}{2. Validation Set}{35}{}%
\contentsline {paragraph}{3. Test Set}{35}{}%
\contentsline {subsection}{\numberline {2.3.2}Data Leakage}{36}{}%
\contentsline {section}{\numberline {2.4}Python Ecosystem}{37}{}%
\contentsline {subsection}{\numberline {2.4.1}Core Scientific Libraries}{37}{}%
\contentsline {subsection}{\numberline {2.4.2}Visualization Tools}{39}{}%
\contentsline {subsection}{\numberline {2.4.3}scikit-learn: The Standard ML Toolkit}{39}{}%
\contentsline {subsection}{\numberline {2.4.4}Linear and Logistic Regression in Python}{40}{}%
\contentsline {chapter}{\numberline {3}Machine Learning Algorithms}{44}{}%
\contentsline {section}{\numberline {3.1}Decision Trees}{45}{}%
\contentsline {paragraph}{At each node of the tree:}{46}{}%
\contentsline {paragraph}{Impurity measure for regression.}{46}{}%
\contentsline {paragraph}{Choosing the best split.}{46}{}%
\contentsline {paragraph}{Recursive growth.}{47}{}%
\contentsline {section}{\numberline {3.2}Random Forest}{50}{}%
\contentsline {section}{\numberline {3.3}Gradient Boosting}{55}{}%
\contentsline {section}{\numberline {3.4}XGBoost: Extreme Gradient Boosting}{59}{}%
\contentsline {section}{\numberline {3.5}Summary: Recognizing and Distinguishing the Algorithms}{65}{}%
\contentsline {chapter}{\numberline {4}Neural Networks \& Deep Learning}{66}{}%
\contentsline {section}{\numberline {4.1}The Perceptron: The Simplest Neural Unit}{66}{}%
\contentsline {section}{\numberline {4.2}Multi–Layer Perceptron (MLP)}{67}{}%
\contentsline {section}{\numberline {4.3}Forward Pass and Loss Function}{68}{}%
\contentsline {section}{\numberline {4.4}Backpropagation: How Neural Networks Learn}{68}{}%
\contentsline {section}{\numberline {4.5}Training with Gradient Descent}{69}{}%
\contentsline {section}{\numberline {4.6}Why Deep Networks Work}{69}{}%
\contentsline {section}{\numberline {4.7}Overfitting and Regularization in Neural Networks}{70}{}%
\contentsline {section}{\numberline {4.8}Python Example: Neural Network for Classification (Keras)}{71}{}%
\contentsline {section}{\numberline {4.9}Summary: Recognizing Neural Networks}{72}{}%
\contentsline {chapter}{\numberline {5}Practical ML}{74}{}%
