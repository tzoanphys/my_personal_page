\babel@toc {english}{}\relax 
\contentsline {chapter}{Introduction}{3}{}%
\contentsline {chapter}{\numberline {1}Statistics You Need for Machine Learning}{4}{}%
\contentsline {section}{\numberline {1.1}Types of Variables}{4}{}%
\contentsline {section}{\numberline {1.2}Descriptive Statistics}{4}{}%
\contentsline {subsection}{\numberline {1.2.1}Mean}{4}{}%
\contentsline {subsection}{\numberline {1.2.2}Median}{5}{}%
\contentsline {subsection}{\numberline {1.2.3}Variance}{5}{}%
\contentsline {subsection}{\numberline {1.2.4}Standard Deviation}{5}{}%
\contentsline {subsection}{\numberline {1.2.5}Covariance}{5}{}%
\contentsline {subsection}{\numberline {1.2.6}Correlation}{5}{}%
\contentsline {section}{\numberline {1.3}Probability Theory Basics}{6}{}%
\contentsline {section}{\numberline {1.4}Bayes' Theorem: Updating Beliefs from Data}{7}{}%
\contentsline {subsection}{\numberline {1.4.1}Formula}{7}{}%
\contentsline {section}{\numberline {1.5}Likelihood}{8}{}%
\contentsline {section}{\numberline {1.6}Probability Distributions: Shape of Data}{10}{}%
\contentsline {subsection}{\numberline {1.6.1}Discrete Distributions}{10}{}%
\contentsline {paragraph}{Bernoulli Distribution}{11}{}%
\contentsline {paragraph}{Binomial Distribution}{11}{}%
\contentsline {paragraph}{Poisson Distribution}{11}{}%
\contentsline {subsection}{\numberline {1.6.2}Continuous Distributions}{11}{}%
\contentsline {paragraph}{Gaussian (Normal Distribution)}{12}{}%
\contentsline {paragraph}{Exponential Distribution}{12}{}%
\contentsline {paragraph}{Multivariate Gaussian}{12}{}%
\contentsline {section}{\numberline {1.7}Statistical Inference: Learning Parameters From Data}{13}{}%
\contentsline {subsection}{\numberline {1.7.1}Maximum Likelihood Estimation (MLE)}{13}{}%
\contentsline {subsection}{\numberline {1.7.2}Maximum A Posteriori Estimation (MAP)}{14}{}%
\contentsline {section}{\numberline {1.8}Linear Regression: Statistical Interpretation}{16}{}%
\contentsline {section}{\numberline {1.9}Logistic Regression: Probabilistic Classification}{18}{}%
\contentsline {section}{\numberline {1.10}Biasâ€“Variance Tradeoff: Why Models Fail}{19}{}%
\contentsline {chapter}{\numberline {2}Python for Machine Learning}{26}{}%
\contentsline {section}{\numberline {2.1}Supervised Data}{26}{}%
\contentsline {subsection}{\numberline {2.1.1}The Form of Supervised Data}{26}{}%
\contentsline {subsection}{\numberline {2.1.2}The Objective of Supervised Learning}{27}{}%
\contentsline {subsection}{\numberline {2.1.3}Regression vs.~Classification}{28}{}%
\contentsline {paragraph}{Regression}{28}{}%
\contentsline {paragraph}{Classification}{28}{}%
\contentsline {subsection}{\numberline {2.1.4}i.i.d.~Assumption}{28}{}%
\contentsline {subsection}{\numberline {2.1.5}Features and Target Variables}{29}{}%
\contentsline {subsection}{\numberline {2.1.6}The Learning Goal}{29}{}%
\contentsline {section}{\numberline {2.2}Loss Function and Risk Minimization}{30}{}%
\contentsline {subsection}{\numberline {2.2.1}Loss for a Single Example}{31}{}%
\contentsline {subsection}{\numberline {2.2.2}Empirical Risk: Total Loss on the Dataset}{31}{}%
\contentsline {subsection}{\numberline {2.2.3}Risk Minimization}{32}{}%
\contentsline {subsection}{\numberline {2.2.4}Example: Linear Regression Loss}{32}{}%
\contentsline {subsection}{\numberline {2.2.5}Example: Logistic Regression Loss}{33}{}%
\contentsline {subsection}{\numberline {2.2.6}Regularization: Controlling Model Complexity}{34}{}%
\contentsline {subsection}{\numberline {2.2.7}Summarize}{35}{}%
\contentsline {section}{\numberline {2.3}Train / Validation / Test}{35}{}%
\contentsline {subsection}{\numberline {2.3.1}The Three Splits}{35}{}%
\contentsline {paragraph}{1. Training Set}{35}{}%
\contentsline {paragraph}{2. Validation Set}{36}{}%
\contentsline {paragraph}{3. Test Set}{36}{}%
\contentsline {subsection}{\numberline {2.3.2}Data Leakage}{37}{}%
\contentsline {section}{\numberline {2.4}Python Ecosystem}{38}{}%
\contentsline {subsection}{\numberline {2.4.1}Core Scientific Libraries}{38}{}%
\contentsline {subsection}{\numberline {2.4.2}Visualization Tools}{40}{}%
\contentsline {subsection}{\numberline {2.4.3}scikit-learn: The Standard ML Toolkit}{40}{}%
\contentsline {subsection}{\numberline {2.4.4}Linear and Logistic Regression in Python}{41}{}%
\contentsline {chapter}{\numberline {3}Machine Learning Algorithms}{45}{}%
\contentsline {section}{\numberline {3.1}Decision Trees}{46}{}%
\contentsline {paragraph}{At each node of the tree:}{47}{}%
\contentsline {paragraph}{Impurity measure for regression.}{47}{}%
\contentsline {paragraph}{Choosing the best split.}{47}{}%
\contentsline {paragraph}{Recursive growth.}{48}{}%
\contentsline {section}{\numberline {3.2}Random Forest}{51}{}%
\contentsline {section}{\numberline {3.3}Gradient Boosting}{56}{}%
\contentsline {section}{\numberline {3.4}XGBoost: Extreme Gradient Boosting}{60}{}%
\contentsline {section}{\numberline {3.5}Summary: Recognizing and Distinguishing the Algorithms}{66}{}%
\contentsline {chapter}{\numberline {4}Neural Networks \& Deep Learning}{67}{}%
\contentsline {section}{\numberline {4.1}The Perceptron: The Simplest Neural Unit}{67}{}%
\contentsline {section}{\numberline {4.2}Multi--Layer Perceptron (MLP)}{70}{}%
\contentsline {section}{\numberline {4.3}Forward Pass and Loss Function}{72}{}%
\contentsline {section}{\numberline {4.4}Backpropagation: How Neural Networks Learn}{72}{}%
\contentsline {section}{\numberline {4.5}Training with Gradient Descent}{73}{}%
\contentsline {section}{\numberline {4.6}Why Deep Networks Work}{75}{}%
\contentsline {section}{\numberline {4.7}Initialization and Training Dynamics}{76}{}%
\contentsline {paragraph}{Xavier (Glorot) Initialization}{76}{}%
\contentsline {paragraph}{He Initialization}{77}{}%
\contentsline {section}{\numberline {4.8}Overfitting and Regularization in Neural Networks}{78}{}%
\contentsline {section}{\numberline {4.9}Python Example: Neural Network for Classification (Keras)}{78}{}%
\contentsline {section}{\numberline {4.10}Summary: Recognizing Neural Networks}{81}{}%
\contentsline {chapter}{\numberline {5}Practical ML}{83}{}%
\contentsline {section}{\numberline {5.1}The Practical Machine Learning Pipeline}{83}{}%
\contentsline {section}{\numberline {5.2}Evaluation Metrics}{83}{}%
\contentsline {section}{\numberline {5.3}Hyperparameter Tuning and Model Selection}{84}{}%
\contentsline {paragraph}{Grid Search}{84}{}%
\contentsline {paragraph}{Random Search}{84}{}%
\contentsline {paragraph}{Bayesian Optimization}{84}{}%
\contentsline {section}{\numberline {5.4}Data Preprocessing Cheatsheet}{85}{}%
\contentsline {chapter}{Annex: Machine Learning Interview Q\&A}{86}{}%
