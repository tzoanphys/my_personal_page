\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Statistics You Need for Machine Learning}{4}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Types of Variables}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Descriptive Statistics}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Mean}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Median}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Variance}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Standard Deviation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Covariance}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.6}Correlation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Probability Theory Basics}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Bayes' Theorem: Updating Beliefs from Data}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Formula}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Likelihood}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Probability Distributions: Shape of Data}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Discrete Distributions}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bernoulli Distribution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Binomial Distribution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Poisson Distribution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}Continuous Distributions}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gaussian (Normal Distribution)}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exponential Distribution}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multivariate Gaussian}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Statistical Inference: Learning Parameters From Data}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Maximum Likelihood Estimation (MLE)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Maximum A Posteriori Estimation (MAP)}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Linear Regression: Statistical Interpretation}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Logistic Regression: Probabilistic Classification}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Biasâ€“Variance Tradeoff: Why Models Fail}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Python for Machine Learning}{26}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Supervised Data}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Form of Supervised Data}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Objective of Supervised Learning}{27}{}\protected@file@percent }
\newlabel{eq:observed_supervised_learning}{{2.1}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Regression vs.~Classification}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}i.i.d.~Assumption}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Features and Target Variables}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}The Learning Goal}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Loss Function and Risk Minimization}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Loss for a Single Example}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Empirical Risk: Total Loss on the Dataset}{31}{}\protected@file@percent }
\newlabel{eq:emprirical_risk}{{2.4}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Risk Minimization}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Example: Linear Regression Loss}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Example: Logistic Regression Loss}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Regularization: Controlling Model Complexity}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Summarize}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Train / Validation / Test}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Three Splits}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Training Set}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Validation Set}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Test Set}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Data Leakage}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Python Ecosystem}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Core Scientific Libraries}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Visualization Tools}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}scikit-learn: The Standard ML Toolkit}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Linear and Logistic Regression in Python}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning Algorithms}{45}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Decision Trees}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{At each node of the tree:}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impurity measure for regression.}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choosing the best split.}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recursive growth.}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Random Forest}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Diagram illustrating the Random Forest algorithm and majority voting process.}}{52}{}\protected@file@percent }
\newlabel{fig:randomforest_diagram}{{3.1}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Gradient Boosting}{56}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Diagram illustrating the Gradient Boosting. }}{56}{}\protected@file@percent }
\newlabel{fig:randomforest_diagram}{{3.2}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}XGBoost: Extreme Gradient Boosting}{60}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Recognizing and Distinguishing the Algorithms}{66}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural Networks \& Deep Learning}{67}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Perceptron: The Simplest Neural Unit}{67}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Multi--Layer Perceptron (MLP)}{70}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  This composite diagram illustrates the three levels of a Deep Learning model: \emph  {(Left)} The architecture of a Multi-Layer Perceptron (MLP) with input ($x$), hidden ($h$), and output ($y$) layers. \emph  {(Top Right)} The internal mechanism of a single perceptron, computing the weighted sum $z = w^\top x + b$ followed by an activation. \emph  {(Bottom Right)} Common activation functions ($\sigma $)---Sigmoid, Tanh, and ReLU---that introduce the necessary nonlinearity into the network. }}{71}{}\protected@file@percent }
\newlabel{fig:randomforest_diagram}{{4.1}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Forward Pass and Loss Function}{72}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Backpropagation: How Neural Networks Learn}{72}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Training with Gradient Descent}{73}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {From Neuron to Network: The Anatomy of Deep Learning.} Visualizing the hierarchy of a neural network: individual inputs ($x$) are processed by neurons using weights ($w$) and biases ($b$), then stacked into layers to form a complete Deep Learning architecture.}}{74}{}\protected@file@percent }
\newlabel{fig:randomforest_diagram}{{4.2}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Why Deep Networks Work}{75}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Deep Learning as a Hierarchy of Representations.} Visualizing the "Simple $\rightarrow $ Abstract" pipeline. While the Input Layer receives raw data ($x$), the sequence of Hidden Layers functions as a progressive filter. Early layers detect simple patterns (like edges), while deeper layers combine these into high-level concepts, allowing the network to learn a complex representation of the data before making a final prediction ($y$).}}{75}{}\protected@file@percent }
\newlabel{fig:hierarchy_of_representations}{{4.3}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Initialization and Training Dynamics}{76}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Xavier (Glorot) Initialization}{76}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{He Initialization}{77}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Overfitting and Regularization in Neural Networks}{78}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Python Example: Neural Network for Classification (Keras)}{78}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}Summary: Recognizing Neural Networks}{81}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Practical ML}{83}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The Practical Machine Learning Pipeline}{83}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Evaluation Metrics}{83}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Hyperparameter Tuning and Model Selection}{84}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Grid Search}{84}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Random Search}{84}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bayesian Optimization}{84}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Data Preprocessing Cheatsheet}{85}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{Annex: Machine Learning Interview Q\&A}{86}{}\protected@file@percent }
\gdef \@abspage@last{97}
