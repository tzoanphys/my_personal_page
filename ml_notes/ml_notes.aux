\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Introduction}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Statistics You Need for Machine Learning}{4}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Types of Variables}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Descriptive Statistics}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Mean}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Median}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Variance}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Standard Deviation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.5}Covariance}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Correlation}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Probability Theory Basics}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Joint Probability}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Conditional Probability}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Marginal Probability}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Bayes' Theorem: Updating Beliefs from Data}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Formula}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Likelihood}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Probability Distributions: Shape of Data}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Discrete Distributions}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bernoulli Distribution}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Binomial Distribution}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Poisson Distribution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Continuous Distributions}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gaussian (Normal Distribution)}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exponential Distribution}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multivariate Gaussian}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Statistical Inference: Learning Parameters From Data}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.1}Maximum Likelihood Estimation (MLE)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8.2}Maximum A Posteriori Estimation (MAP)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Linear Regression: Statistical Interpretation}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Logistic Regression: Probabilistic Classification}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Bias–Variance Tradeoff: Why Models Fail}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Python for Machine Learning}{25}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Supervised Data}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Form of Supervised Data}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The Objective of Supervised Learning}{26}{}\protected@file@percent }
\newlabel{eq:observed_supervised_learning}{{2.1}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Regression vs.~Classification}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Regression}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Classification}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}i.i.d.~Assumption}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Features and Target Variables}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}The Learning Goal}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Loss Function and Risk Minimization}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Loss for a Single Example}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Empirical Risk: Total Loss on the Dataset}{30}{}\protected@file@percent }
\newlabel{eq:emprirical_risk}{{2.4}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Risk Minimization}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Example: Linear Regression Loss}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Example: Logistic Regression Loss}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Regularization: Controlling Model Complexity}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Summarize}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Train / Validation / Test}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The Three Splits}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Training Set}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Validation Set}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Test Set}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Data Leakage}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Python Ecosystem}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Core Scientific Libraries}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Visualization Tools}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}scikit-learn: The Standard ML Toolkit}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Linear and Logistic Regression in Python}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning Algorithms}{44}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Decision Trees}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{At each node of the tree:}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impurity measure for regression.}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Choosing the best split.}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recursive growth.}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Random Forest}{50}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Diagram illustrating the Random Forest algorithm and majority voting process.}}{51}{}\protected@file@percent }
\newlabel{fig:randomforest_diagram}{{3.1}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Gradient Boosting}{55}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Diagram illustrating the Gradient Boosting. }}{55}{}\protected@file@percent }
\newlabel{fig:gradient_boosting_diagram}{{3.2}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}XGBoost: Extreme Gradient Boosting}{59}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Recognizing and Distinguishing the Algorithms}{65}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural Networks \& Deep Learning}{66}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}The Perceptron: The Simplest Neural Unit}{66}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Multi–Layer Perceptron (MLP)}{67}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Forward Pass and Loss Function}{68}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Backpropagation: How Neural Networks Learn}{68}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Training with Gradient Descent}{69}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Why Deep Networks Work}{69}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Overfitting and Regularization in Neural Networks}{70}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {From Neuron to Network: The Anatomy of Deep Learning.} Visualizing the hierarchy of a neural network: individual inputs ($x$) are processed by neurons using weights ($w$) and biases ($b$), then stacked into layers to form a complete Deep Learning architecture.}}{71}{}\protected@file@percent }
\newlabel{fig:neural_network_diagram}{{4.1}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Python Example: Neural Network for Classification (Keras)}{71}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Summary: Recognizing Neural Networks}{72}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Practical ML}{74}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{75}
